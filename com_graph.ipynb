{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Node(object):\n",
    "  \"\"\"\n",
    "  表示具体的数值或者某个Op的数据结果。\n",
    "  \"\"\"\n",
    "  global_id = -1\n",
    "  \n",
    "  def __init__(self, op, inputs):\n",
    "    self.inputs = inputs # 产生该Node的输入\n",
    "    self.op = op # 产生该Node的Op\n",
    "    self.grad = 0.0 # 初始化梯度\n",
    "    self.evaluate() # 立即求值\n",
    "    # 调试信息\n",
    "    self.id = Node.global_id\n",
    "    Node.global_id += 1\n",
    "    print(\"eager exec: %s\" % self)\n",
    "  \n",
    "  def input2values(self):\n",
    "    \"\"\" 将输入统一转换成数值，因为具体的计算只能发生在数值上 \"\"\"\n",
    "    new_inputs = []\n",
    "    for i in self.inputs:\n",
    "      if isinstance(i, Node):\n",
    "        i = i.value\n",
    "      new_inputs.append(i)\n",
    "    return new_inputs\n",
    "\n",
    "  def evaluate(self):\n",
    "    self.value = self.op.compute(self.input2values())\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__str__()\n",
    "\n",
    "  def __str__(self):\n",
    "    return \"Node%d: %s %s = %s, grad: %.3f\" % (\n",
    "      self.id, self.input2values(), self.op.name(), self.value, self.grad)\n",
    "\n",
    "class Op(object):\n",
    "  \"\"\"\n",
    "  所有操作的基类。注意Op本身不包含状态，计算的状态保存在Node中，每次调用Op都会产生一个Node。\n",
    "  \"\"\"\n",
    "  def name(self):\n",
    "    pass\n",
    "  \n",
    "  def __call__(self):\n",
    "    \"\"\" 产生一个新的Node，表示此次计算的结果 \"\"\"\n",
    "    pass\n",
    "\n",
    "  def compute(self, inputs):\n",
    "    \"\"\" Op的计算 \"\"\"\n",
    "    pass\n",
    "\n",
    "  def gradient(self, output_grad):\n",
    "    \"\"\" 计算梯度 \"\"\"\n",
    "    pass\n",
    "\n",
    "class AddOp(Op): \n",
    "  \"\"\"加法运算\"\"\"\n",
    "  def name(self):\n",
    "    return \"add\"\n",
    "\n",
    "  def __call__(self, a, b):\n",
    "    return Node(self, [a, b])\n",
    "  \n",
    "  def compute(self, inputs):\n",
    "    return inputs[0] + inputs[1]\n",
    "  \n",
    "  def gradient(self, inputs, output_grad):\n",
    "    return [output_grad, output_grad] # gradient of a and b\n",
    "\n",
    "class SubOp(Op): \n",
    "  \"\"\"减法运算\"\"\"\n",
    "  def name(self):\n",
    "    return \"sub\"\n",
    "\n",
    "  def __call__(self, a, b):\n",
    "    return Node(self, [a, b])\n",
    "  \n",
    "  def compute(self, inputs):\n",
    "    return inputs[0] - inputs[1]\n",
    "  \n",
    "  def gradient(self, inputs, output_grad):\n",
    "    return [output_grad, -output_grad]\n",
    "\n",
    "class MulOp(Op): \n",
    "  \"\"\"乘法运算\"\"\"\n",
    "  def name(self):\n",
    "    return \"mul\"\n",
    "  \n",
    "  def __call__(self, a, b):\n",
    "    return Node(self, [a, b])\n",
    "  \n",
    "  def compute(self, inputs):\n",
    "    return inputs[0] * inputs[1]\n",
    "\n",
    "  def gradient(self, inputs, output_grad):\n",
    "    return [inputs[1] * output_grad, inputs[0] * output_grad]\n",
    "\n",
    "class LnOp(Op): \n",
    "  \"\"\"自然对数运算\"\"\"\n",
    "  def name(self):\n",
    "    return \"ln\"\n",
    "\n",
    "  def __call__(self, a):\n",
    "    return Node(self, [a])\n",
    "\n",
    "  def compute(self, inputs):\n",
    "    return math.log(inputs[0])\n",
    "  \n",
    "  def gradient(self, inputs, output_grad):\n",
    "    return [1.0/inputs[0] * output_grad]\n",
    "\n",
    "class SinOp(Op): \n",
    "  \"\"\"正弦运算\"\"\"\n",
    "  def name(self):\n",
    "    return \"sin\"\n",
    "\n",
    "  def __call__(self, a):\n",
    "    return Node(self, [a])\n",
    "\n",
    "  def compute(self, inputs):\n",
    "    return math.sin(inputs[0])\n",
    "  \n",
    "  def gradient(self, inputs, output_grad):\n",
    "    return [math.cos(inputs[0]) * output_grad]\n",
    "\n",
    "class IdentityOp(Op): \n",
    "  \"\"\"输入输出一样\"\"\"\n",
    "  def name(self):\n",
    "    return \"identity\"\n",
    "\n",
    "  def __call__(self, a):\n",
    "    return Node(self, [a])\n",
    "\n",
    "  def compute(self, inputs):\n",
    "    return inputs[0]\n",
    "  \n",
    "  def gradient(self, inputs, output_grad):\n",
    "    return [output_grad]\n",
    "\n",
    "class Executor(object):\n",
    "  \"\"\" 计算图的执行和自动微分 \"\"\"\n",
    "  def __init__(self, root):\n",
    "    self.topo_list = self.__topological_sorting(root) # 拓扑排序的顺序就是正向求值的顺序\n",
    "    self.root = root\n",
    "\n",
    "  def run(self):\n",
    "    \"\"\"\n",
    "    按照拓扑排序的顺序对计算图求值。注意：因为我们之前对node采用了eager模式（简单的串行函数调用方式），\n",
    "    实际上每个node值之前已经计算好了，但为了演示lazy计算的效果，这里使用拓扑\n",
    "    排序又计算了一遍。\n",
    "    \"\"\"\n",
    "    node_evaluated = set() # 保证每个node只被求值一次\n",
    "    print(\"\\nEVALUATE ORDER:\")\n",
    "    for n in self.topo_list:\n",
    "      if n not in node_evaluated:\n",
    "        n.evaluate()\n",
    "        node_evaluated.add(n)\n",
    "        print(\"evaluate: %s\" % n)\n",
    "    \n",
    "    return self.root.value\n",
    "\n",
    "  def __dfs(self, topo_list, node):\n",
    "    if Node == None or not isinstance(node, Node):\n",
    "      return\n",
    "    for n in node.inputs:\n",
    "      self.__dfs(topo_list, n)\n",
    "    topo_list.append(node) # 同一个节点可以添加多次，他们的梯度会累加\n",
    "\n",
    "  def __topological_sorting(self, root):\n",
    "    \"\"\"拓扑排序：采用DFS方式\"\"\"\n",
    "    lst = []\n",
    "    self.__dfs(lst, root)\n",
    "    return lst\n",
    "\n",
    "  def gradients(self):\n",
    "    reverse_topo = list(reversed(self.topo_list)) # 按照拓扑排序的反向开始微分\n",
    "    reverse_topo[0].grad = 1.0 # 输出节点梯度是1.0\n",
    "    for n in reverse_topo:\n",
    "      grad = n.op.gradient(n.input2values(), n.grad)\n",
    "      # 将梯度累加到每一个输入变量的梯度上\n",
    "      for i, g in zip(n.inputs, grad):\n",
    "        if isinstance(i, Node):\n",
    "          i.grad += g\n",
    "    print(\"\\nBackward AUTODIFF:\")\n",
    "    for n in reverse_topo:\n",
    "      print(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager exec: Node-1: [2.0] identity = 2.0, grad: 0.000\n",
      "eager exec: Node0: [5.0] identity = 5.0, grad: 0.000\n",
      "eager exec: Node1: [2.0] ln = 0.6931471805599453, grad: 0.000\n",
      "eager exec: Node2: [2.0, 5.0] mul = 10.0, grad: 0.000\n",
      "eager exec: Node3: [0.6931471805599453, 10.0] add = 10.693147180559945, grad: 0.000\n",
      "eager exec: Node4: [5.0] sin = -0.9589242746631385, grad: 0.000\n",
      "eager exec: Node5: [10.693147180559945, -0.9589242746631385] sub = 11.652071455223084, grad: 0.000\n",
      "[Node-1: [2.0] identity = 2.0, grad: 0.000, Node1: [2.0] ln = 0.6931471805599453, grad: 0.000, Node-1: [2.0] identity = 2.0, grad: 0.000, Node0: [5.0] identity = 5.0, grad: 0.000, Node2: [2.0, 5.0] mul = 10.0, grad: 0.000, Node3: [0.6931471805599453, 10.0] add = 10.693147180559945, grad: 0.000, Node0: [5.0] identity = 5.0, grad: 0.000, Node4: [5.0] sin = -0.9589242746631385, grad: 0.000, Node5: [10.693147180559945, -0.9589242746631385] sub = 11.652071455223084, grad: 0.000]\n",
      "\n",
      "EVALUATE ORDER:\n",
      "evaluate: Node-1: [2.0] identity = 2.0, grad: 0.000\n",
      "evaluate: Node1: [2.0] ln = 0.6931471805599453, grad: 0.000\n",
      "evaluate: Node0: [5.0] identity = 5.0, grad: 0.000\n",
      "evaluate: Node2: [2.0, 5.0] mul = 10.0, grad: 0.000\n",
      "evaluate: Node3: [0.6931471805599453, 10.0] add = 10.693147180559945, grad: 0.000\n",
      "evaluate: Node4: [5.0] sin = -0.9589242746631385, grad: 0.000\n",
      "evaluate: Node5: [10.693147180559945, -0.9589242746631385] sub = 11.652071455223084, grad: 0.000\n",
      "y=11.652\n",
      "\n",
      "Backward AUTODIFF:\n",
      "Node5: [10.693147180559945, -0.9589242746631385] sub = 11.652071455223084, grad: 1.000\n",
      "Node4: [5.0] sin = -0.9589242746631385, grad: -1.000\n",
      "Node0: [5.0] identity = 5.0, grad: 1.716\n",
      "Node3: [0.6931471805599453, 10.0] add = 10.693147180559945, grad: 1.000\n",
      "Node2: [2.0, 5.0] mul = 10.0, grad: 1.000\n",
      "Node0: [5.0] identity = 5.0, grad: 1.716\n",
      "Node-1: [2.0] identity = 2.0, grad: 5.500\n",
      "Node1: [2.0] ln = 0.6931471805599453, grad: 1.000\n",
      "Node-1: [2.0] identity = 2.0, grad: 5.500\n",
      "x1.grad=5.500\n",
      "x2.grad=1.716\n"
     ]
    }
   ],
   "source": [
    "# 开始验证程序\n",
    "add, mul, ln, sin, sub, identity = AddOp(), MulOp(), LnOp(), SinOp(), SubOp(), IdentityOp()\n",
    "x1, x2 = identity(2.0), identity(5.0)\n",
    "y = sub(add(ln(x1), mul(x1, x2)), sin(x2)) # y = ln(x1) + x1*x2 - sin(x2)\n",
    "ex = Executor(y)\n",
    "print(ex.topo_list)\n",
    "#前向计算网络输出\n",
    "print(\"y=%.3f\" % ex.run())\n",
    "# 反向计算 自动微分\n",
    "ex.gradients() \n",
    "print(\"x1.grad=%.3f\" % x1.grad)\n",
    "print(\"x2.grad=%.3f\" % x2.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deee9092494de3f243baafa6e8f13ed865fb0b4f248d8311ed438590712b6c6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
